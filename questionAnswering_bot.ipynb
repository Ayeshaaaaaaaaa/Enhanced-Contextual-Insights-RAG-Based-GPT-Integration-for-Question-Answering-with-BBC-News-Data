{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN_XNxyOuRhG"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain transformers faiss-cpu datasets torchvision torchaudio sentence-transformers\n"
      ],
      "metadata": {
        "id": "ulHbIl98u_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-huggingface\n"
      ],
      "metadata": {
        "id": "07QguLkbvBic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data_path = '/content/cleaned_bbc_news_articless.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Combine the columns\n",
        "def combine_columns(row, include_date=True):\n",
        "    if include_date:\n",
        "        return f\"{row['cleaned_title']} {row['cleaned_text']} Published on: {row['pubDate']}\"\n",
        "    else:\n",
        "        return f\"{row['cleaned_title']} {row['cleaned_text']}\"\n",
        "\n",
        "df['combined_text'] = df.apply(combine_columns, axis=1)\n",
        "articles = df['combined_text'].tolist()\n",
        "\n",
        "# SentenceTransformer for embedding\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "embeddings = model.encode(articles, convert_to_tensor=True)  # This will use GPU if available\n",
        "\n",
        "# FAISS Index creation\n",
        "embedding_dim = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "faiss_index.add(np.array(embeddings.cpu()))  # Ensure embeddings are on CPU for FAISS\n",
        "\n",
        "# Initialize the HuggingFace embedding model for LangChain\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Create Document objects for each article, adding a source identifier\n",
        "documents = [Document(page_content=article, metadata={'source': f'Article {i}'}) for i, article in enumerate(articles)]\n",
        "\n",
        "# Create the FAISS vector store from documents\n",
        "vector_store = FAISS.from_documents(documents, hf_embeddings)\n",
        "\n",
        "# Save the FAISS index and embeddings\n",
        "torch.save(embeddings.cpu(), 'bbc_embeddings.pt')  # Ensure embeddings are on CPU for saving\n",
        "faiss.write_index(faiss_index, 'bbc_faiss_index.faiss')\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2-large\"  # Use GPT-2 large\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "gpt_model.to(device)  # Move the model to the appropriate device\n",
        "\n",
        "# Define the pipeline for GPT-2 text generation\n",
        "generative_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gpt_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300,  # Larger max_length for more detailed answers\n",
        "    min_length=100,  # Ensure the answer isn't too short\n",
        "    length_penalty=1.2,\n",
        "    num_beams=4,\n",
        "    truncation=True,\n",
        "    no_repeat_ngram_size=2,  # Prevent repetitive answers\n",
        "    do_sample=True,  # Sampling for creative answers\n",
        "    device=0 if torch.cuda.is_available() else -1  # Adjust for GPU or CPU\n",
        ")\n",
        "\n",
        "#  function to retrieve relevant documents and generate an answer with article publish dates\n",
        "def get_generative_answer(query, vector_store, num_docs=3):\n",
        "    # Retrieve the top `num_docs` relevant documents using FAISS\n",
        "    docs = vector_store.similarity_search(query, k=num_docs)\n",
        "\n",
        "    # Summarize and combine the content of the retrieved documents into a structured context\n",
        "    context = \"\"\n",
        "    for doc in docs:\n",
        "        article_text = doc.page_content[:300]  # Reduce the length of each document snippet\n",
        "        publish_date = doc.metadata.get('source')  # Retrieve the 'source' metadata for the publish date\n",
        "        context += f\"Article published on {publish_date}:\\n{article_text}\\n\\n\"\n",
        "\n",
        "    # Prepare the input prompt for GPT-2\n",
        "    prompt = (\n",
        "        f\"Question: {query}\\n\"\n",
        "        f\"Context: Please provide a detailed answer based on the following articles. Include references to the publishing dates of the articles mentioned.\\n\"\n",
        "        f\"{context}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate the answer using GPT-2 with improved generation control\n",
        "    generated_text = generative_pipeline(\n",
        "        prompt,\n",
        "        max_length=600,  # Increase max_length for more detailed responses\n",
        "        min_length=200,  # Ensure the answer has enough substance\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.6,  # Lower temperature for more focused generation\n",
        "        num_beams=6,  # Increase beams for more thoughtful generation\n",
        "        no_repeat_ngram_size=2  # Prevent repetition\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "# Example query\n",
        "# Allow the user to input a query\n",
        "user_query = input(\"Please enter your query: \")\n",
        "\n",
        "# Get the answer based on the user's query\n",
        "answer = get_generative_answer(user_query, vector_store)\n",
        "\n",
        "# Print the generated answer\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scF6GfCWB_n5",
        "outputId": "59bd1609-c9f2-400c-beda-08bdd51d21a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your query: tell me about russia ukraine war\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Question: tell me about russia ukraine war\n",
            "Context: Please provide a detailed answer based on the following articles. Include references to the publishing dates of the articles mentioned.\n",
            "Article published on Article 26:\n",
            "ukraine war kherson mariupol key russian success strategic historical reason offensive southern ukraine vital russia Published on: 04/03/2022 12:54\n",
            "\n",
            "Article published on Article 302:\n",
            "ukraine happened day russia invasion vladimir putin pave way foreign fighter join war russia widens attack Published on: 11/03/2022 21:12\n",
            "\n",
            "Article published on Article 8256:\n",
            "ukraine war happening russia total fear lithuania russian opposing vladimir putin war say home country like huge prison Published on: 22/09/2022 23:06\n",
            "\n",
            "\n",
            "Answer: The war in Ukraine began on April 12, 2014, when Russia annexed the Crimean peninsula from Ukraine. The conflict has since claimed the lives of more than 6,000 people and displaced millions of others. Russia's annexation of Crimea has been condemned by the United States, the European Union, and the Organization for Security and Co-operation in Europe (OSCE) as a violation of international law and a threat to international peace and security. Russian President Vladimir Putin has said that Russia will not recognize the results of a referendum in Crimea on whether to join Russia or remain part of Ukraine, which is scheduled to take place on March 16. In response, Ukraine's President Petro Poroshenko has called on Russia to withdraw its forces from the peninsula and to allow international monitors to monitor the situation in the region. On April 16, Russia and Ukraine agreed to a cease-fire that was to last for 72 hours, but the truce was violated by both sides, resulting in a new round of fighting between Ukrainian forces and Russian-backed separatists in eastern Ukraine that continues to this day. As a result of this conflict, over 4,500 people have been killed, including over 1,200 civilians, according to United Nations estimates. Ukraine and Russia have accused each other of committing war crimes and crimes against humanity during the conflict. According to Human Rights Watch (HRW), Russian forces have used cluster munitions in populated areas, while the Ukrainian military has used indiscriminate weapons, such as artillery and mortars, against civilian areas. HRW has also documented the use of torture and other cruel, inhuman, or degrading treatment or punishment by Ukrainian security forces, as well as by pro-Russian separatists, on a large number of civilians in Donetsk and Luhansk regions. For more information, please visit: http://www.hrw.org/sites/default/files/documents/human-rights-report-2014-03-16-en.pdf#page=1#mediaviewer/File:HumanRights_Report_2014_03_16_en_Ukrainian_Security_forces_and_\n"
          ]
        }
      ]
    }
  ]
}